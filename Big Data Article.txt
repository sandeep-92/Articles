“You can have data without information, but you cannot have information without data.” – Daniel Keys Moran Billy Beane, the general manager of Oakland Athletics hires a young graduate from Yale university to build a baseball team. But faced with limited budget and departures of players in his team he decides to go the data science way and by using sophisticated sabermetric approach towards scouting and analyzing players eventually builds a team of undervalued players, who go on toe to toe with large market teams. This same algorithm is then used by Red Sox to win 2004 World Series. This is the story of moneyball based on a nonfiction books by Michael Lewis. In today’s time having data at your disposal is considered a huge advantage against those without it. But it’s not just about having data, the focus has shifted to analyzing it and making it useful for betterment of society to ease our day to day lives and in many other fields. Eric Schmidt of Google said “There were 5 exabytes of information created between the dawn of civilization and 2003, but that much data is generated every 2 days”. Today data is being generated at an immense speed, every individual with a smartphone or a desktop is contributing to it. Earlier data was stored in physical forms in papers or register, since nowadays data is generated at an exponential pace, old systems to track it failed to keep up with it, and the failure of old methods paved the path for new technologies and new terminologies. One of that term we commonly hear on internet and media these days is Big data. What is big data? Is it just a generic term or does it have an actual meaning, can we treat all data that’s coming in huge numbers as big data? The answer to that question is yes and no. Yes, huge volume of data can be considered as big data but there are other parameters at play too. They are called the four V’s “ Volume, Variety, Velocity and Veracity. Volume stands for the huge amount of data that we can’t usually store in minimum storage units, here data is not just in mbs or gbs it’s in tera, peta or even exabytes. Variety means the data can be structured, unstructured or semi-structured it can be anything from a table of customer details to video or mp3 files. Velocity is the speed at which data is being generated for processing. From posting a picture on Instagram, liking a post on Facebook to even performing our daily small Google searches generates data and the arrival of smartphones and IoT has increased the speed at which it is being generated. Veracity refers to the abnormalities in data gathered, the trustworthiness of it. Not all data is always complete and useful, some part of it can be redundant or corrupted, so we have to refine it according to our needs. If a dataset satisfies all the four V’s then it is considered as a Big data. The arrival of Big data changed the way organisations work and it redefined their goals. Governments in many countries now use Big data to make policies that affects their citizens directly. It’s help in solving healthcare problems are considered a big positive. In Israel the government has come up with a plan where they will analyze all their citizens medical records and then make medicines according to their body structure, so instead of buying generic drugs over the counter the citizen can get prescriptions made just for them that will work with their body better. But how do we analyse big data? how do we make all this sea of data around us useful? The legacy databases that were being used weren’t able to comprehend the volume and complexity of data. The advancement in storage solution has not been very monumental. In 1990, one drive could save 1370 mb of data and had transfer rate of 4.4mb/s, so all data could be read in 5 mins. Over 25 years later, 1Tb is the normal norm for drives, and transfer speed is 100mbps, so it takes two and half hours to read all the data from the disk and writing is even slower. Seek time of drives are increasing more slowly than transfer time, it characterizes latency of disk operations. The new technology of SSD (solid state drives) solves some of the storage problems but the cost associated with it is not feasible for every organisation. But, the problem isn't with just storing huge data, when it came to processing the huge datasets in relational database management system there was a problem with scaling. As the size of data grew RDMS couldn’t keep up with the demand. You couldn't just add more systems to the cluster and improve the performance, as it doesn’t support linear scaling. So we needed a system where data could be stored on distributed system but with failure tolerance. A system that could process huge data volumes in parallel to improve the time it took to perform the operations. Doug Cutting and Mike Cafarella answered this by developing Hadoop in 2005. At that time they were working on a system to support distribution for Nutch search engine. The technology was based on white papers published by Google known as GFS. Hadoop is a open-source platform that helps in storing and parallel processing of data. It’s meant to run on commodity hardware (easily available hardware from different vendors). It stores the data on distributed systems and lets user perform analysis on the data in parallel. It has come a long way since it arrived in 2005. The latest version of Hadoop is 3.0.0 released in December of 2017, it enhanced and brought many new features to Hadoop environment. Hadoop is a developing technology. It’s still being refined everyday, but it does helps us in tackling the problems of Big data. Hadoop has many components associated with it to solve different problems, like Apache Storm, Apache Spark, Pig, Hive etc. But the core components of Hadoop are HDFS and MapReduce. Hadoop helps us solve many problems associated with legacy databases, like if the operation being performed is very taxing for the cluster of systems, one could always add more systems to the cluster to increase the processing time. But Hadoop is not the answer to everything related data storage and processing it has some limitations of its own. HDFS stands for hadoop distributed filesystem. HDFS stores the data on distributed file systems for processing using commodity hardware. Instead of storing all the data on a single disk it divides the data into chunks known as blocks and stores them all on different datanotes. It’s fault tolerant since it replicates the data on different datanodes, so even if one datanode fails HDFS can always fetch the data from a working datanode and replicate it on a different node if necessary. It works on schema-on-read. It is prefered for writing the file once and reading it many times to perform analysis on whole datasets. HDFS can store structured, unstructured as well as semi-structured data. It is mostly preferred for large files, since lots of small files don’t work too well with it. For large number of small files legacy databases like RDBMS are still prefered. So in short HDFS handles the storing problem of data, the processing part is handled by another entity called MapReduce which is a batch processing system. It runs ad hoc queries on whole datasets or on some portion of it, to give the results in few minutes. It is not meant for interactive analysis. MapReduce has two functions mappers and reducers which work in tandem. The map part takes the job and converts into another set of datasets where individual elements are broken into key-value pairs. The reducers takes the output from mappers as input and combines them in smaller set of tuples. A third component was added to Hadoop 2.x called YARN (yet another resource negotiator), it is a cluster resource management system. It also allows any distributed program to run on data in Hadoop cluster. Most of these environments are hosted by Apache Software Foundations. Organisations like Facebook, Amazon and IBM use hadoop to gain a substantial market advantage over companies not using it. It’s being used for recommendation engines for companies like Netflix, Amazon, Flipkart.By using deep learning and machine learning we are building Artificial Intelligent robots to enhance our productivity. But it’s not just these companies who are being benefited for using Big data. It’s also playing a very crucial role in developing smart cities. It can be implemented to control the traffic flow and avoid accidents. By analysing the peak hours of traffic and the numbers of car, the government can use that data to monitor areas where traffic is a big problem. It can also be applied to check for accident-prone areas and deploy required tools to counter the problem. It can also be used to improve health problems in remote villages. By monitoring the doctors record, diseases that affects the most and can supply required number of medicines. Big data also can play a huge role in online education. It can be used to monitor and analyse any students performance in real-time and then redesign the course to help him better. One early use of Big data was the development of smart electricity meter by IBM in USA. They gathered the records from the meters and checked for the peak usage of electricity by the residents and then directed them to use their electricity efficiently. They told them the peak time at which electricity was used the most and would cost them more. So by only using electricity at non-peak hours they can save a lot of many. These are just few of the use cases of Big data, it’s being used in Banking sector heavily, Healthcare, Agriculture are some the key areas Big data is playing a important role. But there is one area where big data poses a problem for developing nations. The problem is the correct and reliable collections of data from rural areas. Many villages are not connected to Internet, which makes it harder for the government to collect data and perform analysis, as physical records are not always correct or kept routinely. By connecting the post offices, colleges,schools to the internet the government can resolve this issue. Big data has the power to impact every part of our lives in coming years and it will only grow from here. The sooner we realize it and harness its potential the better we can benefit our society with it.